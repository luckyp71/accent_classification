{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee67dfe6-e05f-4575-bc58-5077919b13be",
   "metadata": {},
   "source": [
    "# Accent Classification Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08676f0b-c305-4544-ac6a-b22ed96ce0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucky/anaconda3/envs/python3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-10 18:22:35.423121: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-10 18:22:35.431344: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-10 18:22:35.440406: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-10 18:22:35.443023: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-10 18:22:35.451130: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-10 18:22:35.939845: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, Audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8699f75-054e-4f60-a29a-2c9f4fe32e0e",
   "metadata": {},
   "source": [
    "### Device Agnostic Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d438111b-4888-435a-b5cf-71cdf7950588",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4516d3-1766-4501-9bbf-ef8016951934",
   "metadata": {},
   "source": [
    "### Load Base Model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ba8396-e5a9-438b-9e71-077bc6bf46b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d25188-39e1-4930-9d23-6b0fd7feccf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForSequenceClassification(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projector): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (classifier): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fba68d4-8118-463c-9ab6-1ef9cc34267d",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e9b66ca-89b8-4f70-97aa-121e6d121baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "accent_dirs = dict(american=0, british=1, australian=2)\n",
    "id_to_label = {v: k for k, v in accent_dirs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "825e2783-0df6-434f-b8ff-073ddc100ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label = id_to_label\n",
    "model.config.label2id = accent_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d3ca59f-da82-4db1-bd1f-ad27509dccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for accent, label in accent_dirs.items():\n",
    "    directory = f\"./data/{accent}/\"\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.mp3'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            data.append({\"path\": filepath, \"label\": label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ef9a2a-cbe5-46f7-be70-2c5648c47485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and convert path column to audio format\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset = dataset.cast_column(\"path\", Audio())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92876b-5fa1-4a6d-84fd-8e11af86d726",
   "metadata": {},
   "source": [
    "### Split Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcbf12a5-a8a2-4794-85c0-61898429d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = dataset.train_test_split(test_size=0.2).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51379d79-93df-4abb-b734-874d6f343cd2",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec2887a5-4df7-404c-8396-21e2ceecd5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    audio = batch[\"path\"][\"array\"]\n",
    "    max_length = 176256\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "    \n",
    "    batch[\"input_values\"] = inputs.input_values[0]\n",
    "    \n",
    "    batch[\"labels\"] = torch.tensor([batch[\"label\"]])\n",
    "    \n",
    "    if \"attention_mask\" in inputs:\n",
    "        batch[\"attention_mask\"] = inputs.attention_mask[0]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02b5c9dc-87eb-46d4-9f66-335544e2729c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████| 360/360 [00:04<00:00, 73.35 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████| 91/91 [00:01<00:00, 89.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(preprocess, remove_columns=[\"path\"])\n",
    "test_dataset = test_dataset.map(preprocess, remove_columns=[\"path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09734518-7e79-48ac-b99c-0ca4e827605f",
   "metadata": {},
   "source": [
    "### Set TrainArgs and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea305336-aaf1-4ad9-9c3c-3d57ef452a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_steps=1,        \n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a6c13bf-b880-49ec-baac-d5830a2360f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca7e714-2f47-41db-adca-0cc5468f7ed7",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0f5f875-a4d6-49c2-a7c1-c0c8436eb94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 42:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.092600</td>\n",
       "      <td>1.081394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.050400</td>\n",
       "      <td>1.057964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.004400</td>\n",
       "      <td>1.017192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.963600</td>\n",
       "      <td>0.970215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.932200</td>\n",
       "      <td>0.916304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.903600</td>\n",
       "      <td>0.904914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.870500</td>\n",
       "      <td>0.913313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.832200</td>\n",
       "      <td>0.901563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.832100</td>\n",
       "      <td>0.892076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.825600</td>\n",
       "      <td>0.925454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=230, training_loss=0.9307173936263374, metrics={'train_runtime': 2585.1993, 'train_samples_per_second': 1.393, 'train_steps_per_second': 0.089, 'total_flos': 3.600377602163712e+17, 'train_loss': 0.9307173936263374, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846cb5e7-a8ea-4c7d-96a6-510253bdde8b",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85c26e7b-669f-48ba-bd43-78375e16aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    report = classification_report(labels, predictions, target_names=accent_dirs.keys())\n",
    "    print(report)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbf8386e-ea22-47bc-9369-0268e8331580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    american       0.33      0.09      0.15        32\n",
      "     british       0.46      0.78      0.58        23\n",
      "  australian       0.65      0.78      0.71        36\n",
      "\n",
      "    accuracy                           0.54        91\n",
      "   macro avg       0.48      0.55      0.48        91\n",
      "weighted avg       0.49      0.54      0.48        91\n",
      "\n",
      "{'eval_loss': 0.9254543781280518, 'eval_model_preparation_time': 0.0016, 'eval_accuracy': 0.5384615384615384, 'eval_f1': 0.4786456360518447, 'eval_precision': 0.491471557661182, 'eval_recall': 0.5384615384615384, 'eval_runtime': 10.1892, 'eval_samples_per_second': 8.931, 'eval_steps_per_second': 0.589}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101bed6c-15e6-475c-a146-9485e33b0ee9",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdaa72c2-52a1-406d-bbb7-542dbd14d2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./final_model\")\n",
    "processor.save_pretrained(\"./final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce35f70-d22e-4308-8a6b-80585a6766f9",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9842a375-8c90-4ee0-87cf-d578a0583bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./final_model\"\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_path)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaf034c-0ed3-45df-b165-3b4e860b362c",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "133d6e17-8f88-4af2-90f3-a6c833ff244f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForSequenceClassification(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projector): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (classifier): Linear(in_features=256, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bea0377-00eb-4c07-b402-33792389929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cf3e6d8-7611-4314-9408-b06a9e05cb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: british probability: 0.5170\n"
     ]
    }
   ],
   "source": [
    "audio_file = \"./data/british/common_voice_en_41917501.mp3\" \n",
    "waveform, sample_rate = librosa.load(audio_file, sr=16000)\n",
    "inputs = processor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "probs = F.softmax(logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "predicted_prob = probs[predicted_class]\n",
    "\n",
    "id_to_label = model.config.id2label\n",
    "print(f\"Predicted class: {id_to_label[predicted_class]} probability: {predicted_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f62aa8e-f61d-47f0-856d-a03a974451ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
